<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>NeRF or Nothin'</title>
  <style>
    /* Basic Reset */
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      color: #333;
      background-color: #f4f4f4;
      padding: 20px;
    }

    h1, h2 {
      color: #444;
    }

    h1 {
      font-size: 36px;
      margin-bottom: 20px;
    }

    h2 {
      font-size: 24px;
      margin-bottom: 10px;
      margin-top: 20px;
    }

    p {
      margin-bottom: 10px;
    }

    a {
      color: #2980b9;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .video-container {
      position: relative;
      padding-bottom: 56.25%;
      padding-top: 30px;
      height: 0;
      overflow: hidden;
      margin-bottom: 20px;
    }

    .video-container iframe,
    .video-container object,
    .video-container embed {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
    }

        /* Image Container */
    .image-container {
      position: relative;
      overflow: hidden;
      margin-bottom: 20px;
    }

    .image-container img {
      display: block;
      max-width: 100%;
      height: auto;
    }

    /* Image Caption */
    .image-caption {
      font-size: 14px;
      color: #666;
      text-align: center;
      margin-bottom: 20px;
    }

    /* Flexbox styles for side-by-side videos */
    .video-row {
      display: flex;
      flex-wrap: wrap;
      justify-content: space-between;
      margin-bottom: 5px; /* Adjust gap between video row and capture description */
    }

    .video-row section {
      flex: 0 0 calc(50% - 10px);
    }

    @media screen and (max-width: 768px) {
      .video-row section {
        flex: 0 0 100%;
        margin-bottom: 20px;
      }
    }

    /* Capture description box */
    .capture-description {
      background-color: #fff;
      padding: 20px;
      border-radius: 5px;
      box-shadow: 1px 1px 3px rgba(0, 0, 0, 0.1);
      margin-bottom: 20px;
    }

    .customIndent {
      padding-left: 50px;
    }
    .row {
    display: flex;
    flex-wrap: wrap;
    justify-content: flex-start;
    align-items: flex-start;
    margin: -10px; /* Adjust this value according to the desired gap between items */
  }

  .row > * {
    flex: 1;
    min-width: 300px; /* Adjust this value according to the minimum width you want for each item */
    box-sizing: border-box;
    padding: 10px; /* Adjust this value according to the desired gap between items */
  }

  /* Update the section style */
  section {
    flex-grow: 1;
    background-color: #fff;
    padding: 20px;
    border-radius: 5px;
    box-shadow: 1px 1px 3px rgba(0, 0, 0, 0.1);
    margin-bottom: 20px;
    /* The rest of your section styles */
  }
  .image-row {
    display: flex;
    flex-wrap: wrap;
    justify-content: center;
    align-items: flex-start;
    gap: 10px; /* Adjust this value according to the desired gap between images */
  }

  .image-row img {
    display: block;
    max-width: 100%;
    height: auto;
    box-sizing: border-box;
  }
    /* Image Column */
    .image-column {
    display: flex;
    flex-direction: column;
    align-items: center;
    gap: 10px; /* Adjust this value according to the desired gap between images */
  }

  .image-column img {
    display: block;
    max-width: 100%;
    height: auto;
    box-sizing: border-box;
  }
  .centered-image {
    display: block;
    margin-left: auto;
    margin-right: auto;
  }
  </style>
</head>
<body>
  <h1>Nerf Or Nothin'</h1>
  <section>
    <h2>Team Members</h2>
    <p>
        <ul class="customIndent">
            <li>Zachary Gordon</li>
            <li>Gaurav Rao</li>
            <li>Eric Yoo</li>
        </ul>
    </p>
  </section>
  <section>
    <h3>
      A few Notes
    </h3>
    <p>
      If you are reading this in the PDF version or the local html version you can also click <a href="https://zgordounc.github.io/">here</a> to see it hosted on github pages. It looks better than the PDF and ensures that none of the media is broken while viewing the report.
    </p>
  </section>
  <section>
    <h2>Introduction</h2>
    <p>
        Witness the magic of 3D reconstruction as we demonstrate our efforts to bring UNC Chapel Hill's campus to the digital world - because when it comes to showing off the beauty of our campus, it's NeRF or Nothin'!
         The purpose of this project report is to showcase the application of various computer vision tools and techniques in creating 3D reconstructions of objects around the University of North Carolina at Chapel Hill (UNC Chapel Hill) campus. 
         With the restrictions of the COVID-19 pandemic close by in our rearview mirror and the difficulty of attending physical campus tours for out-of-state students, the need for virtual alternatives has become more evident.
          This project aims to validate the possibilities for a virtual medium for "visitations" to the campus through 3D scans, enabling individuals to experience the awe of UNC’s campus wherever they may be.
           By utilizing a variety of computer vision methodologies, we have created 3D reconstructions of various landmarks on the UNC-Chapel Hill campus.
           The following report will detail the methods and tools used in this project, demonstrations of our results, and the challenges we encountered.
    </p>
  </section>
    <section>
      <h2>The Code</h2>
      <p>
          We initially attempted to write our own Nerfstudio code and run it locally and on Google Colab.
           However, we ran into dependency issues. We didn't have access to a local GPU and we could not get the TinyCuda dependencies set up on Google Colab.
            Luckily Nerfstudio provided a <a href="https://colab.research.google.com/github/nerfstudio-project/nerfstudio/blob/main/colab/demo.ipynb#scrollTo=SiiXJ7K_fePG ", target="_blank">Colab notebook</a> for demonstration purposes.
             We were able to upload our own Polycam data, train the NeRF and create the camera paths for the video, using this notebook. This was the only way we could get the code to run.        
      </p>
    </section>


  <section>
    <h2>Background Research</h2>
    <p>
      As is evident by the title of our project, we used NeRF, short for "Neural Radiance Fields," to actually create our digital scenes. NeRF essentially models a 3D scene as a continuous 5D function that maps 3D spatial coordinates and a viewing direction to a radiance value, which is defined as the amount of light emitted or reflected from a surface. This function is represented by a neural network, which is trained on a set of input images and corresponding 3D geometry. The network takes the 3D coordinates and viewing direction as input and produces an output representing the radiance value.
      </p>
      <p>
        The training data is typically obtained by capturing a set of images of a scene from different viewpoints, and the 3D structure of the scene is estimated by using techniques such as structure from motion or multi-view stereo. The neural network is trained to minimize rendering loss, which measures the difference between the predicted radiance values and the ground truth radiance values when the network is used to render an image of the scene from a given viewpoint. To render an image of the scene from a particular viewpoint using the trained network, the network is evaluated at a set of sample points along the rays that pass through each pixel in the image. The radiance values predicted by the network at these sample points are integrated along the rays to produce a final pixel value.
      </p>
      <p>
        NeRF has several advantages, such as being able to handle complex lighting and reflectance effects, as well as recreating photo-realistic views of objects and scenes from a vast multitude of viewpoints, which are useful for applications such as virtual reality and cinema VFX. However, it also requires a lot of computational time and space, as a large amount of training data is usually required and image rendering can often be slow. 
      </p>
      <div class="centered-image">
          <img src="media/nerf.png">
      </div>

      <p>
        We obtained the data used by our NeRF component through LiDAR (Light Detection and Ranging) scans of scenes and objects around UNC. LiDAR scanners typically work by emitting lasers and measuring the time it takes for the beams to bounce back after hitting a surface to create precise 3D point clouds of the environment. Algorithms such as clustering, segmentation, and object recognition are sometimes employed to preprocess the point cloud and remove noise and outliers. 3D models of the environment can then be created with the point clouds using techniques such as surface reconstruction, meshing, or volumetric modeling. 
      </p>
  </section>
  <div class="video-row">
    <section>
      <h2>Project Design</h2>
      <p>
        Our project was conducted by leveraging variety of tools and technologies. To collect the LiDAR scans of objects around UNC, we used the 3D scanning application PolyCam hosted on an iPhone 14 Pro with an in-built LiDAR scanner.
         PolyCam allows users to reconstruct 3D meshes from LiDAR scans, images, camera intrinsic and extrinsic data, as well as depth maps.
          The application offers a "raw data" export option that enabled us to work with the data in other applications, such as training NeRFs.
           It was this seamless integration with our NeRF pipeline that initially drew us to use PolyCam. After scanning an object or scene in the application, the camera poses are globally optimized using ARKit before reconstructing the mesh, which makes the optimized camera poses as good as, if not better than, those obtained from an SfM software like COLMAP. This is particularly helpful for complex indoor scenes where SfM pipelines might fail. As a result, the user does not have to worry about drift while scanning and can scan the same area multiple times in a single session.
      </p>
      <p>
        After collecting the data of a scene or object, the next step was to use Nerfstudio, which has in-built PolyCam data-processing capabilities. Nerfstudio is an open-source library for creating, training, and testing NeRFs. Typically, the NeRF process is very involved and complicated to conduct manually, and at the scale of our project, it would have been extremely difficult. Nerfstudio allowed us to simplify the process by abstracting the steps behind a user interface. After importing the raw data into this program, we were able to train a model and render a 3D scene reconstruction video by specifying a camera position and path. We did this for each subject. All of this was conducted on a Google Colab notebook, as our group members did not have the computational resources to use Nerfstudio in a local setting. Google Colab’s GPU-accelerated capabilities made training the models and rendering the videos much more streamlined of a process than it would have been if we attempted to do the same on our hardware set-up.
      </p>
      <p>
        Our approach for actually shortlisting the objects and scenes to be reconstructed was based on two criteria. We sought to scan and render subjects that held both practical value to individuals learning about UNC and utility in showing off 3D reconstruction in a variety of settings. Firstly, the Old Well was selected as it is perhaps the most prominent UNC landmark, and is located in a relatively unobstructed open space that would allow us to test our scene-building pipeline with, we hoped, few obfuscations. The Mask statue outside of the PlayMakers Repertory Theater was also selected because of its significance as a UNC landmark but allowed us to test reconstruction in a setting with a wall on one side and an open space on the other. We created a scan of our team member Eric by the Student Union to demonstrate a student on campus in an open area with a lot of moving artifacts around him to test the limits of our pipeline, as we also wanted to see what artifacts or unexpected elements could arise in the scene recreations. The student bedroom scene was selected as we wanted to provide insight to students on what a typical off-campus residence could look like, as well as simulate a closed environment with a lot of small but unmoving details. We selected the old computer in Fred Brooks Hall as we wanted to try capturing a single, small, stationary subject that held historical significance in relation to UNC’s accomplishments.
      </p>
    </section>
    <section>
      <h2>Viewer Screenshot</h2>
      <div class="image-container">
        <img src="media/viewer_screenshot.png">
        <div class = "image-caption">
          <p>Screenshot of viewer for training the student apartment</p>
        </div>
      </div>
    </section>
  </div>
  <section>
    <h2>Additional PolyCam captures</h2>
    <p>
      Here are some PolyCam captures that we were excited about but didn't end up rendering due to issues with the code.
    </p>
    <div class="image-row">
      <img src="media/well_1.jpg" width = 250 height="663">
      <img src="media/well_2.jpg" width = 250 height = '663'>
      <img src="media/panels_front.jpg" width = 250 height="663">
      <img src="media/panels_back.jpg" width = 250 height = '663'>
  </section>




  <!-- Video Row - Side by Side Videos -->
  <section>
    <h2>Capture 1: Playmakers Mask</h2>
    <p>
        This mask is located outside of Playmakers Repriatory Company on Country Club Road. We chose this capture because it is a unique object that is not a building or a statue and represents the variaty of art around campus.
        Two videos are shown to demonstrate how different renders and novel views can be created from the same capture. We did this by creating different paths in the Nerfstudio viewer.
        We captured the video by walking around the mask in a circle, pointing the polycam camera at the mask. passes were made both inside and outside the pillars.
         As you can see both videos have artifacts and blurry spots.
        Obfuscation is likely the cause of these artifacts. Obfuscation is when areas are hidden from the camera. When this happens the algorithm has a hard time reconstructing the novel view. More detail on the specific artfacts are under the videos.
    </p>
  </section>
  <div class="video-row">
    <section>
      <h2>Playmakers Mask Zoom</h2>
      <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/iWhc5C56uhI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
      <p>
        A simple zoom-in, zoom-out camera path was specified for this rendering. The reflections on the window behind the mask were captured really well. The mask itself and the area between the two pillars are captured in high detail, with very little noise and very few blurry artifacts. However, towards the left and the right side of the pillars and in front of the mask where the camera starts, there seems to be a lot of space with random colored gas-like artifacts, causing high levels of blurriness and discontinuity. The right pillar looks almost deconstructed or broken, and the poster above the mask is blurry and hard to read. Essentially, the render did a really good job of capturing the subject, which in this case was the mask, and otherwise was less detailed than ideal.
      </p>
    </section>

    <section>
      <h2>Playmakers Mask Rotation</h2>
      <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/f3tniUNm6RA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
      <p>
        The camera path goes around the pillars surrounding the mask from the right to the left, while staying oriented to the mask the entire time. The mask itself is captured in high detail while surrounding objects are not as clear. This is likely due to the fact that our LiDAR scan of the mask was focused mainly on the mask itself, and the surrounding imagery was only captured by the images recorded during scanning. As the camera moves around the mask, we can see how the reflection of the sunlight off of the mask’s forehead and nose is well recreated as the reflections follow the path of the camera’s movement. This property of maintaining observation angle and corresponding real-world light reflections is one of the chief advantages of using NeRF to recreate scenes. The reflections of images in the window behind the mask also display similar accuracy in view-dependent effects. As we move from the right side of the mask to behind the mask, we can see that the trees and foliage in the background have been recorded well in terms of depth and motion-parallax accuracy, but the small details like individual leaves and branches have not been captured as well. Directly behind the mask, there are several floating stick-like unintended artifacts in a row that seem to be made of the same material as the pillars. We surmise that these artifacts are the result of obfuscations. As we come around to the left of the subject, we can see several gas-like artifacts in front of the mask’s face. These were likely created by inconsistencies in data capture as someone walked past during scanning. Close observation of that general area as the camera moves around reveals the distinguishable outline of a person who later moved away, leaving behind unwanted artifacts in the final render. This is a good example of how moving elements in the field of view during data capture can throw off the neural network, leading to blurry or noisy artifacts in the result.
      </p>
    </section>
  </div>
  <section>
    <h2>Polycam Captures of the Playmakers Mask</h2>
    <p>
      We wanted to show the captures we used to create the videos. Here are screenshots of PolyCam's 3D renders from different views.
    </p>
    <div class = 'image-row'>
      <img src = 'media/mask_front.jpg' width = 250 height="663">
      <img src = 'media/mask_back.jpg' width = 250 height="663">
      <img src = 'media/mask_right.jpg' width = 250 height="663">
      <img src = 'media/mask_left.jpg' width = 250 height="663">
    </div>
  </section>
  <Section>
    <h2>Capture 2: Student next to the Union</h2>
    <p>
      This capture is located outside the student Union. The point of the capture is to demonstrate how well NeRF can recreate humans. It also shows a student in a campus environmnet which supports the idea NeRF can be used for tours.
    </p>
  </Section>
  <div class="video-row">
    <section>
      <h2>Student next to the Union</h2>
      <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/KVSzk9zbtzM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
      <p>
        In this scan, there were not many reflective areas. However, the lighting conditions were very unidirectional, and so the angle of shadows and subject lighting remained consistent throughout the render. Throughout the rotation around the subject, Eric remained highly detailed, and it was possible to discern the subject texture at all angles. Motion parallax remained consistent as well, but background objects that were further away were not clearly detailed, such as the bike stands and building wall textures. While we can make out shapes, we cannot make out defined borders on objects. The reflections on the windows of Davis Library (dark red brick building) were captured consistently with the camera angle, as can be seen as the camera rotates around. Surrounding Eric is a lot of noise and general blurriness, most likely caused by the network attempting to resolve inconsistencies with the movement of other individuals around Eric. 
      </p>
    </section>
    <section>
      <h2>Artifact in View</h2>
      <div class = 'image-container'>
          <img src="media/eric.png" alt="Student standing outside the Union" width="560" height="315">
      </div>
      <p>
        Outline of another student can be seen from this novel viewpoint. This artifact points out the inconsistencies of the capture.
      </p>
    </section>
  </div>
  <section>
    <h2>Capture 3: Student Apartment</h2>
    <p>
      This capture is located in a student apartment. The point of the capture is to demonstrate how well NeRF can recreate indoor environments. It also shows a student in a campus environmnet which supports the idea NeRF can be used for tours.
    </p>
  </section>
  <section>
    <h2>Student Apartment</h2>
    <div class="video-container">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/lGs2yGdR6Xg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
    </div>
    <p>
      The subject and lower regions of this capture are recorded in relatively very high detail, as we can make out the textures on the floor, sofas, flowers, and many other objects in the scene. The transparency of the windows and coffee table is also captured really well, as is the reflection of the outdoor light on the coffee table at the center of the scene. The background gets blurrier as the camera swivels around to face the opposite direction that it started in, likely because there were moving individuals in the background in that direction as the scan was taking place. This is a good example of how moving elements during data capture can mess up the final render, as we can directly contrast the blurry areas in front of the TV and kitchen to the crisp, detailed areas by the windows and sofas where there were no moving elements during scanning. One thing that is evident throughout the video is the existence of lots of noise and random artifacts toward the top of the scene. We are unsure exactly how this was created, but one theory we had was that the properties of the transparent glass coffee table may have interfered with the LiDAR scanner by causing the LiDAR rays to not reflect as intended, or to reflect off of the surface in unintended ways.
    </p>
  </section>
  <section>
    <h2>Capture 4: Really Old Computer</h2>
  </section>
  <section>
    <h2>Fred Brooks Computer</h2>
    <div class="video-container">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/1Q9Kbq1uyMc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
    </div>
    <p>
      This is a pretty simple scene capture, as the camera simply moves from down to up while staying focused on a central point on the computer. What this capture did really well was to portray the depth of the computer components through a reflective glass pane. We can see the computer internals in relatively good detail, while still being able to make out the existence of trees behind the camera in the reflection of the glass pane. However, likely due to this reflectivity, we can see many stray artifacts around the camera’s path that are similar in color to the reflected trees. Other than these artifacts, there are almost no other instances of unintended elements in the scene rendering, likely owing to the fact that there were no moving elements in the scene during scanning. This shows us that in a controlled environment with minimal moving parts, it is possible to get near-perfect scene recreations with very few unintended artifacts. If not for the reflection-based artifacts in this scene, we may have been able to create a rendering with almost no stray artifacts. Such an observation could guide future scans and NeRF endeavors.
    </p>

    
  </section>

  <!-- Add more video rows as needed -->
  <section>
    <h2>Challenges and Complications</h2>
    <p>
      One of the first challenges we ran into was figuring out how to use Nerfstudio. Due to the massive computational power required, we needed to find a way to carry out the pipeline on a system that could handle it. Since none of our group members had a computer with a GPU powerful enough to do this, we searched for alternative strategies and eventually settled upon using the Nerfstudio-created Google Colab notebook. This seemed to work fine for our purposes. 
    </p>
    <p>
      Initially, our project focus was to use NeRF to construct manipulable 3D scenes of several types of coral specimens for future use in marine science research endeavors, but we ran into several roadblocks along the way. For context, we were given a large image bank containing many pictures of each type of coral from a multitude of angles, covering all necessary viewpoints and perspectives. Using these images, our next step would have been to perform camera calibration to estimate the intrinsic and extrinsic parameters of the camera used to capture the images (including camera poses and focal length), generate depth maps for each image using structure from motion or another stereo reconstruction algorithm, and create 2D-3D correspondences before the data was ready to be used in training the neural network in Nerfstudio.
    </p>
    <p>
      However, we ran into complications trying to get the demo notebook or anything else to work with regular images. We believe this is due to our inability to get COLMAP running in our environment. COLMAP uses structure from motion (SFM) to calculate the camera poses. At this point, we briefly attempted to locally build our own NeRF pipeline independent of Nerfstudio. However, this quickly proved difficult. Since we couldn't get this to work we outsourced the camera pose calculations to PolyCam, which uses LiDAR to compute the camera poses. Using Polycam meant we couldn't use the coral reef images, since we needed to get our own captures.
    </p>
    <p>
      We decided to pivot to the current approach of collecting novel captures from around campus. Yet even using PolyCam we had trouble getting the code to run. At first, the Nerfstudio viewer, which is used to create the camera paths, would suddenly disconnect the rendering process. At some point during the project timeline, the viewer UI changed and we were able to get a few successful video renders without too many changes on our end. We believe that Nerfstudio updated the library, which allowed us to complete the task. Unfortunately, the code stopped working again towards the end of the project. This meant we could not render all of our captures, including that of the Old Well. We have attached the data of all our captures, including the ones that we could not fully render, to this project. Having access to a powerful local GPU would have mitigated many of the challenges that we faced, due to the ease and increased control over the process on our own devices.
    </p>
    <div class="image-row">
      <img src="media/broken_viewer.png">
      <div class="image-caption">
        <p>
          What the Nerfstudio viewer looked like when training didn't work.
        </p>
  </section>

  <section>
    <h2>Implications and Future Study</h2>
    <p>
      As evidenced by our project, the proof of concept exists for virtually recreating real-world scenes using NeRF across a variety of settings, and there are important implications for student engagement and accessibility. By creating a virtual format to experience UNC’s landmarks and lifestyles, we are providing an innovative and accessible way for students to connect with their campus environment, regardless of physical proximity or mobility restrictions. This project also has the potential to enhance recruitment and marketing efforts by showcasing the noteworthy aspects of UNC-Chapel Hill to prospective students who may be on the fence about applying or planning a visit to campus. While the results we obtained are not flawless, further adjustments and better data-collection strategies in more controlled environments could certainly produce market-worthy video renders. 
    </p>
    <p>
      Overall, our project demonstrated that NeRF can be used to leverage the power of virtual technology to create meaningful and engaging experiences, with implications for education, tourism, and beyond. Museums, libraries, and other settings of academic interest could create detailed virtual guided tours to share knowledge with a wider audience. Tourist attractions could create short previews of the experiences they offer to share as marketing material, or could even create detailed virtual environments for paying customers to experience from afar. All of this could be done on a relatively low budget, as we have demonstrated in our project that the backend technology is easily available through open-source means and the data collection can be conducted with just an iPhone.
    </p>
    <p>
      Some things we could have done differently are taking more captures in a greater variety of settings, such as during the night time or in dimly-lit rooms. This would allow us to test the neural network under more unusual situations so that we could report on the outcomes or any issues that would arise in novel contexts. We could also have sourced a more powerful computer for the term of this project to run our code locally so that we could have more control over the specifics. Some things that we could do to expand our own project going forward include taking captures at a larger scale, such as that of an entire building exterior and interior, allowing us to do a walk-through of the building. This may have required better scanning systems as we would need to ensure proper detail capture at an enormous scale, but with the right resources it would be possible. We also could expand our data collection to areas outside of UNC to demonstrate the power of NeRF to new audiences or could partner up with firms such as real-estate and architecture agencies to recreate building interiors and showcase the practical value of our project.
    </p>
    
  </section>
</body>
</html>