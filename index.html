<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Your Report Title</title>
  <style>
    /* Basic Reset */
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      color: #333;
      background-color: #f4f4f4;
      padding: 20px;
    }

    h1, h2 {
      color: #444;
    }

    h1 {
      font-size: 36px;
      margin-bottom: 20px;
    }

    h2 {
      font-size: 24px;
      margin-bottom: 10px;
      margin-top: 20px;
    }

    p {
      margin-bottom: 10px;
    }

    a {
      color: #2980b9;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .video-container {
      position: relative;
      padding-bottom: 56.25%;
      padding-top: 30px;
      height: 0;
      overflow: hidden;
      margin-bottom: 20px;
    }

    .video-container iframe,
    .video-container object,
    .video-container embed {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
    }

        /* Image Container */
    .image-container {
      position: relative;
      overflow: hidden;
      margin-bottom: 20px;
    }

    .image-container img {
      display: block;
      max-width: 100%;
      height: auto;
    }

    /* Image Caption */
    .image-caption {
      font-size: 14px;
      color: #666;
      text-align: center;
      margin-bottom: 20px;
    }

    /* Flexbox styles for side-by-side videos */
    .video-row {
      display: flex;
      flex-wrap: wrap;
      justify-content: space-between;
      margin-bottom: 5px; /* Adjust gap between video row and capture description */
    }

    .video-row section {
      flex: 0 0 calc(50% - 10px);
    }

    @media screen and (max-width: 768px) {
      .video-row section {
        flex: 0 0 100%;
        margin-bottom: 20px;
      }
    }

    /* Capture description box */
    .capture-description {
      background-color: #fff;
      padding: 20px;
      border-radius: 5px;
      box-shadow: 1px 1px 3px rgba(0, 0, 0, 0.1);
      margin-bottom: 20px;
    }

    .customIndent {
      padding-left: 50px;
    }
    .row {
    display: flex;
    flex-wrap: wrap;
    justify-content: flex-start;
    align-items: flex-start;
    margin: -10px; /* Adjust this value according to the desired gap between items */
  }

  .row > * {
    flex: 1;
    min-width: 300px; /* Adjust this value according to the minimum width you want for each item */
    box-sizing: border-box;
    padding: 10px; /* Adjust this value according to the desired gap between items */
  }

  /* Update the section style */
  section {
    flex-grow: 1;
    background-color: #fff;
    padding: 20px;
    border-radius: 5px;
    box-shadow: 1px 1px 3px rgba(0, 0, 0, 0.1);
    margin-bottom: 20px;
    /* The rest of your section styles */
  }
  .image-row {
    display: flex;
    flex-wrap: wrap;
    justify-content: center;
    align-items: flex-start;
    gap: 10px; /* Adjust this value according to the desired gap between images */
  }

  .image-row img {
    display: block;
    max-width: 100%;
    height: auto;
    box-sizing: border-box;
  }
  </style>
</head>
<body>
  <h1>Nerf Or Nothin'</h1>
  <section>
    <h2>Team Members</h2>
    <p>
        <ul class="customIndent">
            <li>Zachary Gordon</li>
            <li>Gaurav Rao</li>
            <li>Eric Yoo</li>
        </ul>
    </p>
  </section>

  <section>
    <h2>Introduction</h2>
    <p>
        Witness the magic of 3D reconstruction as we demonstrate our efforts to bring UNC Chapel Hill's campus to the digital world - because when it comes to showing off the beauty of our campus, it's NeRF or Nothin'!
         The purpose of this project report is to showcase the application of various computer vision tools and techniques in creating 3D reconstructions of objects around the University of North Carolina at Chapel Hill (UNC Chapel Hill) campus. 
         With the restrictions of the COVID-19 pandemic close by in our rearview mirror and the difficulty of attending physical campus tours for out-of-state students, the need for virtual alternatives has become more evident.
          This project aims to validate the possibilities for a virtual medium for "visitations" to the campus through 3D scans, enabling individuals to experience the awe of UNCâ€™s campus wherever they may be.
           By utilizing a variety of computer vision methodologies, we have created 3D reconstructions of various landmarks on the UNC-Chapel Hill campus.
           The following report will detail the methods and tools used in this project, demonstrations of our results, and the challenges we encountered.
    </p>
  </section>
  <div class="video-row">
    <section>
      <h2>The Code</h2>
      <p>
          We initially attempted to write our own Nerfstudio code and run it locally and on Google Colab.
           However, we ran into dependency issues. We didn't have access to a local GPU and we could not get the TinyCuda dependencies set up on Google Colab.
            Luckily Nerfstudio provided a <a href="https://colab.research.google.com/github/nerfstudio-project/nerfstudio/blob/main/colab/demo.ipynb#scrollTo=SiiXJ7K_fePG ", target="_blank">Colab notebook</a> for demonstration purposes.
             We were able to upload our own Polycam data, train the NeRF and create the camera paths for the video, using this notebook. This was the only way we could get the code to run.        
      </p>
    </section>
    <section>
      <h2>Viewer Screenshot</h2>
      <div class="image-container">
        <img src="media/viewer_screenshot.png">
      </div>
    </section>
  </div>

  <section>
    <h2>Background Research</h2>
    <p>
        As is evident by the title of our project, we used NeRF, short for "Neural Radiance Fields," to actually create our digital scenes. At a high level, NeRF models a 3D scene as a continuous 5D function that maps 3D spatial coordinates and a viewing direction to a radiance value (i.e., the amount of light emitted or reflected from a surface). This function is represented by a neural network, which is trained on a set of input images and corresponding 3D geometry. The network takes the 3D coordinates and viewing direction as input and produces an output representing the radiance value.
      </p>
      <p>
        The training data is typically obtained by capturing a set of images of a scene from different viewpoints, and the 3D structure of the scene is estimated by using techniques such as structure from motion or multi-view stereo. The neural network is trained to minimize rendering loss, which measures the difference between the predicted radiance values and the ground truth radiance values when the network is used to render an image of the scene from a given viewpoint.
      </p>
      <p>
        To render an image of the scene from a given viewpoint using the trained network, the network is evaluated at a set of sample points along the rays that pass through each pixel in the image. The radiance values predicted by the network at these sample points are integrated along the rays to produce a final pixel value.
        NeRF has several advantages, such as being able to handle complex lighting and reflectance effects, as well as recreating photo-realistic images of objects and scenes from uncommon viewpoints, which is useful for applications such as virtual reality and visual effects. However, it also requires lots of computational time and space, as a large amount of training data is usually required and image rendering can often be slow.
      </p>
      <p>
        We obtained the data used by our NeRF component through LiDAR (Light Detection and Ranging) scans of scenes and objects around UNC. LiDAR scanners typically work by emitting lasers and measuring the time it takes for the beams to bounce back after hitting a surface to create precise 3D point clouds of the environment. Algorithms such as clustering, segmentation, and object recognition are sometimes employed to preprocess the point cloud and remove noise and outliers. 3D models of the environment can then be created with the point clouds using techniques such as surface reconstruction, meshing, or volumetric modeling.
      </p>
  </section>
  <section>
    <h2>Project Design</h2>
    <p>
      Our project was conducted by leveraging variety of tools and technologies. To collect the LiDAR scans of objects around UNC, we used the 3D scanning application PolyCam hosted on an iPhone 14 Pro with an in-built LiDAR scanner.
       PolyCam allows users to reconstruct 3D meshes from LiDAR scans, images, camera intrinsic and extrinsic data, as well as depth maps.
        The application offers a "raw data" export option that enabled us to work with the data in other applications, such as training NeRFs.
         It was this seamless integration with our NeRF pipeline that initially drew us to use PolyCam. After scanning an object or scene in the application, the camera poses are globally optimized using ARKit before reconstructing the mesh, which makes the optimized camera poses as good as, if not better than, those obtained from an SfM software like COLMAP. This is particularly helpful for complex indoor scenes where SfM pipelines might fail. As a result, the user does not have to worry about drift while scanning and can scan the same area multiple times in a single session.
    </p>
  </section>



  <!-- Video Row - Side by Side Videos -->
  <section>
    <h2>Capture 1: Playmakers Mask</h2>
    <p>
        This mask is located outside of Playmakers Repriatory Company on Country Club Road. We chose this capture because it is a unique object that is not a building or a statue and represents the variaty of art around campus.
        Two videos are shown to demonstrate how different renders and novel views can be created from the same capture. We did this by creating different paths in the Nerfstudio viewer.
        We captured the video by walking around the mask in a circle, pointing the polycam camera at the mask. passes were made both inside and outside the pillars.
         As you can see both videos have artifacts and blurry spots.
        Obfuscation is likely the cause of these artifacts. Obfuscation is when areas are hidden from the camera. When this happens the algorithm has a hard time reconstructing the novel view. More detail on the specific artfacts are under the videos.
    </p>
  </section>
  <div class="video-row">
    <section>
      <h2>Playmakers Mask Zoom</h2>
      <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/iWhc5C56uhI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
      <p>
        A simple zoom-in, zoom-out camera path was specified for this rendering. The reflections on the window behind the mask were captured really well. The mask itself and the area between the two pillars are captured in high detail, with very little noise and very few blurry artifacts. However, towards the left and the right side of the pillars and in front of the mask where the camera starts, there seems to be a lot of space with random colored gas-like artifacts, causing high levels of blurriness and discontinuity. The right pillar looks almost deconstructed or broken, and the poster above the mask is blurry and hard to read. Essentially, the render did a really good job of capturing the subject, which in this case was the mask, and otherwise was less detailed than ideal.
      </p>
    </section>

    <section>
      <h2>Playmakers Mask Rotation</h2>
      <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/f3tniUNm6RA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
      <p>
        The camera path goes around the pillars surrounding the mask from the right to the left, while staying oriented to the mask the entire time. The mask itself is captured in high detail while surrounding objects are not as clear. This is likely due to the fact that our LiDAR scan of the mask was focused mainly on the mask itself, and the surrounding imagery was only captured by the images recorded during scanning. As the camera moves around the mask, we can see how the reflection of the sunlight off of the maskâ€™s forehead and nose is well recreated as the reflections follow the path of the cameraâ€™s movement. This property of maintaining observation angle and corresponding real-world light reflections is one of the chief advantages of using NeRF to recreate scenes. The reflections of images in the window behind the mask also display similar accuracy in view-dependent effects. As we move from the right side of the mask to behind the mask, we can see that the trees and foliage in the background have been recorded well in terms of depth and motion-parallax accuracy, but the small details like individual leaves and branches have not been captured as well. Directly behind the mask, there are several floating stick-like unintended artifacts in a row that seem to be made of the same material as the pillars. We surmise that these artifacts are the result of obfuscations. As we come around to the left of the subject, we can see several gas-like artifacts in front of the maskâ€™s face. These were likely created by inconsistencies in data capture as someone walked past during scanning. Close observation of that general area as the camera moves around reveals the distinguishable outline of a person who later moved away, leaving behind unwanted artifacts in the final render. This is a good example of how moving elements in the field of view during data capture can throw off the neural network, leading to blurry or noisy artifacts in the result.
      </p>
    </section>
  </div>
  <div class="video-row">
    <section>
      <h2>Student next to the Union</h2>
      <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/KVSzk9zbtzM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
      <p>
        In this scan, there were not many reflective areas. However, the lighting conditions were very unidirectional, and so the angle of shadows and subject lighting remained consistent throughout the render. Throughout the rotation around the subject, Eric remained highly detailed, and it was possible to discern the subject texture at all angles. Motion parallax remained consistent as well, but background objects that were further away were not clearly detailed, such as the bike stands and building wall textures. While we can make out shapes, we cannot make out defined borders on objects. The reflections on the windows of Davis Library (dark red brick building) were captured consistently with the camera angle, as can be seen as the camera rotates around. Surrounding Eric is a lot of noise and general blurriness, most likely caused by the network attempting to resolve inconsistencies with the movement of other individuals around Eric. 
      </p>
    </section>
    <section>
      <h2>Artifact in View</h2>
      <div class = 'image-container'>
          <img src="media/eric.png" alt="Student standing outside the Union" width="560" height="315">
      </div>
      <p>
        Outline of another student can be seen from this novel viewpoint. This artifact points out the inconsistencies of the capture.
      </p>
    </section>
  </div>

  <section>
    <h2>Video 1: Video Title</h2>
    <div class="video-container">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/lGs2yGdR6Xg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
    </div>
    <p>
      Replace this text with your description of the capture for Video 1.
    </p>
  </section>
  <section>
    <h2>Video 1: Video Title</h2>
    <div class="video-container">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/1Q9Kbq1uyMc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
    </div>
    <p>
      Replace this text with your description of the capture for Video 1.
    </p>
  </section>

  <!-- Add more video rows as needed -->
  <section>
    <h2>Challenges and Complications</h2>
    <p>
        Our original plan was not to take captures around campus but to construct a video of a coral reef from images taken by marine biologists. However, We could not get the demo notebook, or anything else to work with regular images. We believe this is due to our inability to get COLMAP running in our environment. COLMAP uses structure from motion (SFM) to calculate the camera poses. Since we couldn't get this to work we outsourced the the camera pose calculations to Polycam, which uses lidar to compute the camera poses.
        Using Polycam meant we couldn't use the coral reef images, since we needed to get our own captures. We decided to pivot to collecting novel captures around campus. Even using polycam we had trouble getting the code to run. At first the viewer, which is used to create the camera paths would not work. At some point during the project the UI changed on the viewer and it worked. We believe that Nerfstudio updated the library, which allowed us to complete the assignment.
    </p>
  </section>
  <section>
    <h2>Conclusion</h2>
    <p>
      Replace this text with your conclusion.
    </p>
  </section>
</body>
</html>